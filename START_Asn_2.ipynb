{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m8-Q-57Ivxjq",
        "outputId": "94949fdd-1531-4e01-b652-6e2018adb717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'thinkplot'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1387223364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mthinkplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthinkstats2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'thinkplot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#import thinkplot\n",
        "#import thinkstats2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import scipy.stats as ss\n",
        "import thinkplot\n",
        "import thinkstats2\n",
        "\n",
        "##Seaborn for fancy plots.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcTJZGfGvxjs"
      },
      "source": [
        "<h1>Assignment 2: Life Expectancy and You!</h1>\n",
        "\n",
        "<b>Instructions:</b>\n",
        "<ul>\n",
        "<li>Use the worksheet below to act as a guide to exploring and examining the data in the dataset.\n",
        "<li>Try to make things easy to read. Look at the formatting stuff above.\n",
        "<li>As long as you do what is asked, you can do things in many different ways. You may need to do a little searching.\n",
        "</ul>\n",
        "\n",
        "### Please Use Formatting to Make it Clear What I Should be Looking at! Please!!! I Beg You!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SIEhksjvvxjt",
        "outputId": "d9a58e46-183d-47cc-ae5c-bdf6714ab2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'led.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-519325125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load data and do some cleanup before starting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Please don't change this stuff.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"led.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdfLE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Status\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"isDeveloped\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdfLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"isDeveloped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Developed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Developing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'led.csv'"
          ]
        }
      ],
      "source": [
        "#Load data and do some cleanup before starting\n",
        "#Please don't change this stuff.\n",
        "dfLE = pd.read_csv(\"led.csv\")\n",
        "dfLE.rename(columns={\"Status\":\"isDeveloped\", }, inplace=True)\n",
        "dfLE[\"isDeveloped\"].replace(('Developed', 'Developing'), (1, 0), inplace=True)\n",
        "dfLE = dfLE.drop(columns={\"Year\", \"Incomecompositionofresources\", \"thinness5-9years\", \"HIV/AIDS\", \"percentageexpenditure\", \"Totalexpenditure\", \"thinness1-19years\"})\n",
        "dfLE = dfLE.groupby(\"Country\", as_index=False).mean()\n",
        "dfLE.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM4R9zZyvxju"
      },
      "source": [
        "<h2>Part 1: Add a column for GDP per Capita (5pts)</h2>\n",
        "\n",
        "Show print the labels along with just that column. Print 10 random rows of that data.\n",
        "\n",
        "Note: you'll have to look at the units to ensure you get it right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiR8TVG9vxju"
      },
      "outputs": [],
      "source": [
        "#Part 1\n",
        "\n",
        "dfLE['GDPcap'] = dfLE['GDP'] / dfLE['Population']\n",
        "dfLE.head(10)\n",
        "\n",
        "#print labels\n",
        "print(\"GDP per capita: \")\n",
        "print(dfLE[[\"Country\", \"GDP_per_capita\"]].head())\n",
        "\n",
        "#print 10 random rows\n",
        "print(\"\\nGDP per capita data:\")\n",
        "print(dfLE[[\"Country\", \"GDP_per_capita\"]].sample(10, random_state=42))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGEroTFbvxju"
      },
      "source": [
        "### Part 2: Print a visualization of each distribution. Both regular (density) and cumulative. (15pts)\n",
        "\n",
        "Print each distribution is some type of visualization, also print the cumulative distribution.\n",
        "<ul>\n",
        "<li>5 points are for using some visualization that is effective.\n",
        "<li>5 points for formatting things in a nice layout. Hint: for whatever chart you use, look into some formatting options. Think of what to try to Google for an example for printing multiple charts, a grid of charts, etc... and try to adapt one to yours. There is also a way to do it using some thinkstats/thinkplot stuff.\n",
        "<li><b>5 points are for doing it in an efficient way - just listing off each column is lots of typing... Remember, the #1 trait of programmers is laziness with respect to repetitive tasks. If we are doing the same thing over and over with tiny changes, try to think of a condensed way to do this...</b>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjhWUJdLvxju"
      },
      "outputs": [],
      "source": [
        "#Part 2\n",
        "#Print all distributions\n",
        "#Print all histograms\n",
        "\n",
        "\n",
        "listCol = dfLE.columns\n",
        "fence = 5\n",
        "intervals = 30  # number of intervals\n",
        "\n",
        "for col in listCol:\n",
        "    if col == \"Country\":\n",
        "        continue\n",
        "\n",
        "    # numeric only, drop NaNs\n",
        "    s = pd.to_numeric(dfLE[col], errors=\"coerce\").dropna()\n",
        "    if s.empty:\n",
        "        continue\n",
        "\n",
        "    # trim high outliers (mean * fence)\n",
        "    m = s.mean()\n",
        "    s = s[s < (m * fence)]\n",
        "    if s.empty:\n",
        "        continue\n",
        "\n",
        "    # if too few unique values, skip KDE to avoid warnings\n",
        "    use_kde = s.nunique() > 1\n",
        "\n",
        "    # two-panel figure (density vs cumulative)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
        "\n",
        "    # left: distribution (density)\n",
        "    sns.histplot(\n",
        "        data=s, bins=intervals, stat=\"density\", kde=use_kde,\n",
        "        ax=axes[0], edgecolor=\"black\", alpha=0.7\n",
        "    )\n",
        "    axes[0].set_title(f\"Distribution of {col}\")\n",
        "    axes[0].set_xlabel(col)\n",
        "    axes[0].set_ylabel(\"Density\")\n",
        "\n",
        "    # right: cumulative (CDF)\n",
        "    sns.histplot(\n",
        "        data=s, bins=intervals, stat=\"density\", cumulative=True, kde=False,\n",
        "        ax=axes[1], edgecolor=\"black\", linewidth=1.2\n",
        "    )\n",
        "    axes[1].set_title(f\"Cumulative Distribution (CDF) of {col}\")\n",
        "    axes[1].set_xlabel(col)\n",
        "    axes[1].set_ylabel(\"Cumulative Probability\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPB3A4qvxjv"
      },
      "source": [
        "### Create an analytical distribution for SCHOOLING, and use it for a couple of things. (20pts)\n",
        "\n",
        "<ul>\n",
        "<li>5 points are for choosing an appropriate distribution. Please state it clearly somewhere.\n",
        "<li>10 points are for creating and visualizing the distribution. Do it <b>without</b> using a fit() method. Plot the emperical data on the same graph as the analytical distribution.\n",
        "<li>5 points are for using the distribution to predict the percentage of that have a schooling level within 1 year of Canada.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiHwp7aHvxjv"
      },
      "outputs": [],
      "source": [
        "#Analytical for Schooling\n",
        "\n",
        "\n",
        "print(\"\\nANALYTICAL DISTRIBUTION FOR SCHOOLING\")\n",
        "\n",
        "# Choose normal distribution for schooling\n",
        "# - Schooling is a continuous variable\n",
        "# - It's bounded on both ends (0 years to ~20+ years)\n",
        "\n",
        "print(\"Distribution Choice: Normal Distribution\")\n",
        "print(\"Reason: Schooling represents years of education, which typically follows a normal distribution in large populations\")\n",
        "\n",
        "# Get schooling data and remove missing values\n",
        "schooling_data = dfLE['Schooling'].dropna()\n",
        "print(f\"Schooling data: {len(schooling_data)} countries with valid data\")\n",
        "\n",
        "# Calculate parameters for normal distribution MANUALLY (without using fit())\n",
        "# For normal distribution, we need mean (μ) and standard deviation (σ)\n",
        "mu_schooling = schooling_data.mean()    # μ (mean) - center of distribution\n",
        "sigma_schooling = schooling_data.std()  # σ (standard deviation) - spread of distribution\n",
        "\n",
        "print(f\"Schooling Statistics:\")\n",
        "print(f\"  Mean (μ): {mu_schooling:.2f} years\")\n",
        "print(f\"  Standard Deviation (σ): {sigma_schooling:.2f} years\")\n",
        "print(f\"  Min: {schooling_data.min():.2f} years, Max: {schooling_data.max():.2f} years\")\n",
        "\n",
        "# Create analytical normal distribution using scipy.stats\n",
        "# ss.norm creates a normal distribution with specified mean and standard deviation\n",
        "analytical_schooling = ss.norm(loc=mu_schooling, scale=sigma_schooling)\n",
        "\n",
        "# Plot empirical vs analytical distributions\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot empirical data (actual data from dataset)\n",
        "# This shows what the real distribution looks like\n",
        "plt.hist(schooling_data, bins=30, density=True, alpha=0.7,\n",
        "         color='lightblue', label='Empirical Data', edgecolor='black')\n",
        "\n",
        "# Plot analytical PDF (Probability Density Function)\n",
        "# This shows what our theoretical normal distribution predicts\n",
        "x = np.linspace(schooling_data.min(), schooling_data.max(), 100)  # 100 points from min to max\n",
        "plt.plot(x, analytical_schooling.pdf(x), 'r-', linewidth=2, label='Normal Distribution Fit')\n",
        "\n",
        "plt.xlabel('Schooling (years)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Schooling Distribution: Empirical Data vs Analytical Normal Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Use the distribution to predict percentage of countries within 1 year of Canada's schooling level\n",
        "print(\"\\nUsing distribution for prediction:\")\n",
        "# First, find Canada's schooling level\n",
        "canada_schooling = dfLE[dfLE['Country'] == 'Canada']['Schooling'].values[0]\n",
        "print(f\"Canada's schooling level: {canada_schooling:.2f} years\")\n",
        "\n",
        "# Define the range: within 1 year of Canada's level\n",
        "lower_bound = canada_schooling - 1\n",
        "upper_bound = canada_schooling + 1\n",
        "print(f\"Range: {lower_bound:.2f} to {upper_bound:.2f} years\")\n",
        "\n",
        "# Calculate probability using Cumulative Distribution Function (CDF)\n",
        "# CDF(upper_bound) gives P(X ≤ upper_bound)\n",
        "# CDF(lower_bound) gives P(X ≤ lower_bound)\n",
        "# So P(lower_bound ≤ X ≤ upper_bound) = CDF(upper_bound) - CDF(lower_bound)\n",
        "prob_within_1_year = (analytical_schooling.cdf(upper_bound) - analytical_schooling.cdf(lower_bound)) * 100\n",
        "\n",
        "print(f\"Probability calculation:\")\n",
        "print(f\"  CDF({upper_bound:.2f}) = {analytical_schooling.cdf(upper_bound):.3f}\")\n",
        "print(f\"  CDF({lower_bound:.2f}) = {analytical_schooling.cdf(lower_bound):.3f}\")\n",
        "print(f\"  Difference = {analytical_schooling.cdf(upper_bound) - analytical_schooling.cdf(lower_bound):.3f}\")\n",
        "print(f\"Percentage of countries with schooling within 1 year of Canada: {prob_within_1_year:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kspf7rUHvxjv"
      },
      "source": [
        "### Part 3: Create another analytical distribution for GDP per Capita (20pts)\n",
        "\n",
        "<ul>\n",
        "<li>5 points are for choosing an appropriate distribution. Please state it clearly somewhere.\n",
        "<li>10 points are for creating and visualizing it alongside the emperical. You have free reign on how.\n",
        "<li>5 points are for using the analytical distribution to evaluate the quality of your GDP per capita metric - does it appear accurate? Why or why not?\n",
        "<li>If you've judged GDP per cap to be unreliable, drop it.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx53o47qvxjv"
      },
      "outputs": [],
      "source": [
        "# PART 3\n",
        "\n",
        "#Analytical for GDP/capita\n",
        "\n",
        "\n",
        "print(\"\\nANALYTICAL DISTRIBUTION FOR GDP PER CAPITA\")\n",
        "\n",
        "# Choose log-normal distribution for GDP per capita:\n",
        "# - Economic data (income, wealth) typically follows log-normal distribution\n",
        "# - Cannot have negative values (log-normal is always positive)\n",
        "# - Has a long right tail (few very rich countries, many poorer ones)\n",
        "print(\"Distribution Choice: Log-Normal Distribution\")\n",
        "print(\"Reason: Economic variables like income and wealth typically follow log-normal distribution with positive skew\")\n",
        "\n",
        "# Get GDP per capita data and clean it\n",
        "gdp_data = dfLE['GDP_per_capita'].dropna()  # Remove missing values\n",
        "gdp_data = gdp_data[gdp_data > 0]  # Remove zeros and negative values (log requires positive)\n",
        "print(f\"GDP per capita data: {len(gdp_data)} countries with valid positive data\")\n",
        "\n",
        "# For log-normal distribution, we work with log-transformed data\n",
        "# If X ~ LogNormal, then log(X) ~ Normal\n",
        "log_gdp = np.log(gdp_data)\n",
        "\n",
        "# Calculate parameters for log-normal distribution\n",
        "# For log-normal, we need the mean and std of the log-transformed data\n",
        "mu_log = log_gdp.mean()    # Mean of log(GDP)\n",
        "sigma_log = log_gdp.std()  # Std of log(GDP)\n",
        "\n",
        "print(f\"Log(GDP per capita) Statistics:\")\n",
        "print(f\"  Mean of log(GDP): {mu_log:.2f}\")\n",
        "print(f\"  Std of log(GDP): {sigma_log:.2f}\")\n",
        "print(f\"  Original GDP - Min: ${gdp_data.min():.2f}, Max: ${gdp_data.max():.2f}\")\n",
        "\n",
        "# Create analytical log-normal distribution\n",
        "# ss.lognorm uses s=σ (shape parameter) and scale=exp(μ)\n",
        "analytical_gdp = ss.lognorm(s=sigma_log, scale=np.exp(mu_log))\n",
        "\n",
        "# Plot empirical vs analytical distributions\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot empirical data\n",
        "plt.hist(gdp_data, bins=30, density=True, alpha=0.7,\n",
        "         color='lightgreen', label='Empirical Data', edgecolor='black')\n",
        "\n",
        "# Plot analytical PDF\n",
        "x = np.linspace(gdp_data.min(), gdp_data.max(), 100)\n",
        "plt.plot(x, analytical_gdp.pdf(x), 'r-', linewidth=2, label='Log-Normal Distribution Fit')\n",
        "\n",
        "plt.xlabel('GDP per Capita ($)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('GDP per Capita Distribution: Empirical Data vs Analytical Log-Normal Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEzDnfHCvxjv"
      },
      "source": [
        "### Part 4: Visually identify correlations. Separate developed and not developed countries is some way. (15pts)\n",
        "\n",
        "<ul>\n",
        "<li>5 points are for effectively and clearly showing the visualizations used to evaluate correlations.\n",
        "<li>10 points are for, in the process of visualizing, remove outliers that confound correlations. List outlier \"filters\" you used and justify why - was the data an error, a real value that is just way out of line, or somethingn else? Remember, you're trying to find things that have correlations with each other. Looking at a correlation between home size and income may not benefit from Jeff Bezos being included. It may be helpful to look at the results and revise.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3RXiTMQvxjv"
      },
      "outputs": [],
      "source": [
        "# PART 4\n",
        "#Visualize correlations.\n",
        "\n",
        "print(\"\\nCORRELATION ANALYSIS WITH OUTLIER HANDLING\")\n",
        "\n",
        "# Separate developed and developing countries for comparison\n",
        "# This lets us see if relationships are different between these groups\n",
        "developed = dfLE[dfLE['isDeveloped'] == 1]\n",
        "developing = dfLE[dfLE['isDeveloped'] == 0]\n",
        "\n",
        "print(f\"Country counts: {len(developed)} developed, {len(developing)} developing\")\n",
        "\n",
        "# Remove outliers that could distort correlation analysis\n",
        "print(\"\\nApplying outlier filters to improve correlation analysis:\")\n",
        "print(\"Rationale: Extreme outliers can disproportionately influence correlation coefficients\")\n",
        "\n",
        "# Define reasonable filters based on domain knowledge:\n",
        "filters = (\n",
        "    (dfLE['GDP_per_capita'] > 0) &           # Valid economic data\n",
        "    (dfLE['Lifeexpectancy'] > 30) &          # Biologically plausible minimum\n",
        "    (dfLE['Lifeexpectancy'] < 90) &          # Biologically plausible maximum\n",
        "    (dfLE['AdultMortality'] < 500) &         # Extreme values likely errors\n",
        "    (dfLE['Population'] > 1000)              # Remove microstates with extreme per capita values\n",
        ")\n",
        "\n",
        "print(\"Filters applied:\")\n",
        "print(\"1. GDP_per_capita > 0 (remove invalid/negative economic data)\")\n",
        "print(\"2. Lifeexpectancy between 30-90 years (biologically plausible range)\")\n",
        "print(\"3. AdultMortality < 500 (extreme values likely data errors)\")\n",
        "print(\"4. Population > 1000 (remove microstates with extreme per capita values)\")\n",
        "\n",
        "# Apply filters\n",
        "dfLE_filtered = dfLE[filters]\n",
        "developed_filtered = dfLE_filtered[dfLE_filtered['isDeveloped'] == 1]\n",
        "developing_filtered = dfLE_filtered[dfLE_filtered['isDeveloped'] == 0]\n",
        "\n",
        "print(f\"After filtering: {len(developed_filtered)} developed, {len(developing_filtered)} developing countries\")\n",
        "\n",
        "# Select key variables for correlation analysis\n",
        "# Choosing variables that theoretically should relate to life expectancy\n",
        "key_vars = ['Lifeexpectancy', 'GDP_per_capita', 'Schooling', 'BMI', 'AdultMortality', 'Alcohol']\n",
        "print(f\"\\nAnalyzing correlations for: {key_vars}\")\n",
        "\n",
        "# Create correlation heatmaps for easy comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Developed countries correlation heatmap\n",
        "corr_developed = developed_filtered[key_vars].corr()\n",
        "# sns.heatmap creates a colored grid where color intensity shows correlation strength\n",
        "sns.heatmap(corr_developed, annot=True, cmap='coolwarm', center=0, ax=ax1, fmt='.2f')\n",
        "# annot=True shows correlation values in each cell\n",
        "# cmap='coolwarm' uses blue for negative, red for positive correlations\n",
        "# center=0 sets white color at correlation=0\n",
        "ax1.set_title('Correlation Matrix - Developed Countries')\n",
        "\n",
        "# Developing countries correlation heatmap\n",
        "corr_developing = developing_filtered[key_vars].corr()\n",
        "sns.heatmap(corr_developing, annot=True, cmap='coolwarm', center=0, ax=ax2, fmt='.2f')\n",
        "ax2.set_title('Correlation Matrix - Developing Countries')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create scatter plots to visualize key relationships\n",
        "print(\"\\nCreating scatter plots to visualize relationships...\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: GDP vs Life Expectancy\n",
        "axes[0,0].scatter(developing_filtered['GDP_per_capita'], developing_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developing', color='blue', s=60)\n",
        "axes[0,0].scatter(developed_filtered['GDP_per_capita'], developed_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developed', color='red', s=60)\n",
        "axes[0,0].set_xlabel('GDP per Capita ($)')\n",
        "axes[0,0].set_ylabel('Life Expectancy (years)')\n",
        "axes[0,0].set_title('GDP per Capita vs Life Expectancy')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].set_xscale('log')  # Use log scale because GDP has huge range\n",
        "# Log scale makes exponential relationships appear linear\n",
        "\n",
        "# Plot 2: Schooling vs Life Expectancy\n",
        "axes[0,1].scatter(developing_filtered['Schooling'], developing_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developing', color='blue', s=60)\n",
        "axes[0,1].scatter(developed_filtered['Schooling'], developed_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developed', color='red', s=60)\n",
        "axes[0,1].set_xlabel('Schooling (years)')\n",
        "axes[0,1].set_ylabel('Life Expectancy (years)')\n",
        "axes[0,1].set_title('Schooling vs Life Expectancy')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Plot 3: Adult Mortality vs Life Expectancy\n",
        "axes[1,0].scatter(developing_filtered['AdultMortality'], developing_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developing', color='blue', s=60)\n",
        "axes[1,0].scatter(developed_filtered['AdultMortality'], developed_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developed', color='red', s=60)\n",
        "axes[1,0].set_xlabel('Adult Mortality (per 1000)')\n",
        "axes[1,0].set_ylabel('Life Expectancy (years)')\n",
        "axes[1,0].set_title('Adult Mortality vs Life Expectancy')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Plot 4: BMI vs Life Expectancy\n",
        "axes[1,1].scatter(developing_filtered['BMI'], developing_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developing', color='blue', s=60)\n",
        "axes[1,1].scatter(developed_filtered['BMI'], developed_filtered['Lifeexpectancy'],\n",
        "                 alpha=0.6, label='Developed', color='red', s=60)\n",
        "axes[1,1].set_xlabel('Body Mass Index (BMI)')\n",
        "axes[1,1].set_ylabel('Life Expectancy (years)')\n",
        "axes[1,1].set_title('BMI vs Life Expectancy')\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dt6C-rxvxjv"
      },
      "source": [
        "### Part 5: Calculate correlations (25pts)\n",
        "\n",
        "<ul>\n",
        "<li>5 points are for identifying and calculating 3 strongest correlations with life expectancy for each of developed and not developed.\n",
        "<li>10 points are for analyzing those correlations. Do they appear to be supported by reality? Do you have reason to beleive any are causal?\n",
        "<li>10 points are for identifying 2 places where developing and developed countries have a substantially different relationship with respect to life expectancy, and theorizing why. What would be needed to confirm your thoughts?\n",
        "</ul>\n",
        "\n",
        "<b>Please put your written parts in markdown cells, with a little formatting to make them readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "galipN3Fvxjw"
      },
      "outputs": [],
      "source": [
        "# PART 5\n",
        "\n",
        "#Calculate correlations and analyze.\n",
        "\n",
        "\n",
        "print(\"\\nCORRELATION CALCULATIONS AND ANALYSIS\")\n",
        "\n",
        "# Function to get top correlations with life expectancy\n",
        "def get_top_correlations(data, target_var, n=3):\n",
        "    #Calculate the top n absolute correlations with the target variable.\n",
        "    # data: DataFrame\n",
        "    # target_var: e.g., \"Lifeexpectancy\"\n",
        "    # n: number of top correlations to return\n",
        "\n",
        "\n",
        "    # Select only numeric columns for correlation calculation\n",
        "    numeric_data = data.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Check if target variable exists in numeric data\n",
        "    if target_var not in numeric_data.columns:\n",
        "        raise ValueError(f\"Target variable '{target_var}' not found in numeric columns\")\n",
        "\n",
        "    # Calculate correlation matrix using only numeric data\n",
        "    corr_matrix = numeric_data.corr()\n",
        "\n",
        "    # Get correlations with target variable, take absolute values, sort descending\n",
        "    correlations = corr_matrix[target_var].abs().sort_values(ascending=False)\n",
        "\n",
        "    # Remove target variable itself and isDeveloped (if present)\n",
        "    to_remove = [target_var, 'isDeveloped']\n",
        "    correlations = correlations.drop(to_remove, errors='ignore')\n",
        "\n",
        "    # Return top n correlations\n",
        "    return correlations.head(n)\n",
        "\n",
        "print(\"Calculating top correlations with Life Expectancy...\")\n",
        "\n",
        "# First, let's check what columns we have in our filtered datasets\n",
        "print(f\"Columns in developed_filtered: {list(developed_filtered.columns)}\")\n",
        "print(f\"Columns in developing_filtered: {list(developing_filtered.columns)}\")\n",
        "\n",
        "# Check which columns are numeric\n",
        "numeric_cols_developed = developed_filtered.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols_developing = developing_filtered.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumeric columns in developed: {numeric_cols_developed}\")\n",
        "print(f\"Numeric columns in developing: {numeric_cols_developing}\")\n",
        "\n",
        "# Calculate correlations separately for each group\n",
        "try:\n",
        "    # Top correlations for developed countries\n",
        "    top_developed = get_top_correlations(developed_filtered, 'Lifeexpectancy')\n",
        "    print(\"\\n Top 3 ABSOLUTE correlations with Life Expectancy - DEVELOPED countries:\")\n",
        "    print(top_developed)\n",
        "\n",
        "    # Top correlations for developing countries\n",
        "    top_developing = get_top_correlations(developing_filtered, 'Lifeexpectancy')\n",
        "    print(\"\\n Top 3 ABSOLUTE correlations with Life Expectancy - DEVELOPING countries:\")\n",
        "    print(top_developing)\n",
        "\n",
        "    # Now get actual correlation values (not absolute) to see direction\n",
        "    developed_numeric = developed_filtered.select_dtypes(include=[np.number])\n",
        "    developing_numeric = developing_filtered.select_dtypes(include=[np.number])\n",
        "\n",
        "    developed_corrs = developed_numeric.corr()['Lifeexpectancy'].drop(['Lifeexpectancy', 'isDeveloped'], errors='ignore')\n",
        "    developing_corrs = developing_numeric.corr()['Lifeexpectancy'].drop(['Lifeexpectancy', 'isDeveloped'], errors='ignore')\n",
        "\n",
        "    print(\"\\n ACTUAL correlation values (showing direction):\")\n",
        "    print(\"\\nDeveloped countries:\")\n",
        "    for var in top_developed.index:\n",
        "        if var in developed_corrs:\n",
        "            corr_value = developed_corrs[var]\n",
        "            direction = \"positive\" if corr_value > 0 else \"negative\"\n",
        "            strength = \"strong\" if abs(corr_value) > 0.7 else \"moderate\" if abs(corr_value) > 0.3 else \"weak\"\n",
        "            print(f\"  {var}: {corr_value:.3f} ({strength} {direction})\")\n",
        "\n",
        "    print(\"\\nDeveloping countries:\")\n",
        "    for var in top_developing.index:\n",
        "        if var in developing_corrs:\n",
        "            corr_value = developing_corrs[var]\n",
        "            direction = \"positive\" if corr_value > 0 else \"negative\"\n",
        "            strength = \"strong\" if abs(corr_value) > 0.7 else \"moderate\" if abs(corr_value) > 0.3 else \"weak\"\n",
        "            print(f\"  {var}: {corr_value:.3f} ({strength} {direction})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error calculating correlations: {e}\")\n",
        "    print(\"Troubleshooting: Checking for NaN values and data types...\")\n",
        "\n",
        "    # Debug information\n",
        "    print(f\"Developed countries shape: {developed_filtered.shape}\")\n",
        "    print(f\"Developing countries shape: {developing_filtered.shape}\")\n",
        "    print(f\"NaN values in developed Lifeexpectancy: {developed_filtered['Lifeexpectancy'].isna().sum()}\")\n",
        "    print(f\"NaN values in developing Lifeexpectancy: {developing_filtered['Lifeexpectancy'].isna().sum()}\")\n",
        "\n",
        "# Additional analysis: Compare specific relationships\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" COMPARATIVE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate specific correlations of interest\n",
        "variables_to_compare = ['GDP_per_capita', 'Schooling', 'AdultMortality', 'BMI', 'Alcohol']\n",
        "\n",
        "print(\"\\nComparison of specific correlations:\")\n",
        "print(f\"{'Variable':<15} {'Developed':<10} {'Developing':<10} {'Difference':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for var in variables_to_compare:\n",
        "    if var in developed_numeric.columns and var in developing_numeric.columns:\n",
        "        dev_corr = developed_numeric.corr().loc['Lifeexpectancy', var]\n",
        "        dev_corr = 0 if pd.isna(dev_corr) else dev_corr\n",
        "\n",
        "        dev_corr = developing_numeric.corr().loc['Lifeexpectancy', var]\n",
        "        dev_corr = 0 if pd.isna(dev_corr) else dev_corr\n",
        "\n",
        "        diff = abs(dev_corr) - abs(dev_corr)\n",
        "        print(f\"{var:<15} {dev_corr:>8.3f}   {dev_corr:>8.3f}   {diff:>8.3f}\")\n",
        "\n",
        "# Create a summary visualization of the top correlations\n",
        "print(\"\\n CREATING CORRELATION SUMMARY VISUALIZATION...\")\n",
        "\n",
        "# Get the top variables from both groups\n",
        "top_vars_combined = list(set(list(top_developed.index) + list(top_developing.index)))\n",
        "\n",
        "if top_vars_combined:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Developed countries correlations\n",
        "    dev_corrs = []\n",
        "    dev_labels = []\n",
        "    for var in top_developed.index[:5]:  # Top 5\n",
        "        if var in developed_corrs:\n",
        "            dev_corrs.append(developed_corrs[var])\n",
        "            dev_labels.append(var)\n",
        "\n",
        "    axes[0].barh(dev_labels, dev_corrs, color=['green' if x > 0 else 'red' for x in dev_corrs])\n",
        "    axes[0].set_xlim(-1, 1)\n",
        "    axes[0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    axes[0].set_title('Top Correlations with Life Expectancy\\nDeveloped Countries')\n",
        "    axes[0].set_xlabel('Correlation Coefficient')\n",
        "\n",
        "    # Developing countries correlations\n",
        "    dev_corrs = []\n",
        "    dev_labels = []\n",
        "    for var in top_developing.index[:5]:  # Top 5\n",
        "        if var in developing_corrs:\n",
        "            dev_corrs.append(developing_corrs[var])\n",
        "            dev_labels.append(var)\n",
        "\n",
        "    axes[1].barh(dev_labels, dev_corrs, color=['green' if x > 0 else 'red' for x in dev_corrs])\n",
        "    axes[1].set_xlim(-1, 1)\n",
        "    axes[1].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    axes[1].set_title('Top Correlations with Life Expectancy\\nDeveloping Countries')\n",
        "    axes[1].set_xlabel('Correlation Coefficient')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No top variables found for visualization\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ml3950')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}